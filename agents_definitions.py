#!/usr/bin/env python3
"""
Vulners AI Agent - Agents, Tasks and Tools Definitions

This module contains all agent, task, and tool definitions for the Vulners AI Agent.
Separated from the main execution file for better organization and maintainability.
"""

# =============================================================================
# IMPORTS
# =============================================================================

import os
from crewai import Agent, Task, Crew, Process, LLM
from crewai_tools import MCPServerAdapter, SerperDevTool

import sys
import io
import threading
import time

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Debug configuration - will be overridden by main script
DEBUG_ENABLED = os.getenv("DEBUG", "False").lower() == "true"

# =============================================================================
# LLM CONFIGURATION AND INITIALIZATION
# =============================================================================

def _canonicalize_openai_model_name(name: str) -> str:
    """Normalize OpenAI model names."""
    if not name:
        return "gpt-4o"
    normalized = name.strip().lower()
    alias_map = {
        "gpt4o": "gpt-4o",
        "gpt-4o": "gpt-4o",
        "gpt-4o-mini": "gpt-4o-mini",
        "gpt5": "gpt-5",
        "gpt-5": "gpt-5",
    }
    return alias_map.get(normalized, normalized)

# Configuration variables (can be overridden via environment)
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")  # Can be overridden via environment
MODEL_NAME = _canonicalize_openai_model_name(OPENAI_MODEL)

# =============================================================================
# MCP SERVER CONFIGURATION & TOOL DISCOVERY
# =============================================================================

# Global MCP adapter - keeps connection open for the lifetime of the application
_mcp_adapter = None
_mcp_tools_cache = []

def get_mcp_tools(mcp_url):
    """Get tools from MCP server using CrewAI's built-in MCPServerAdapter.

    Args:
        mcp_url (str): The MCP server URL. Must be provided and non-empty.
    """
    global _mcp_adapter, _mcp_tools_cache

    # Return cached tools if already initialized
    if _mcp_tools_cache:
        return _mcp_tools_cache

    # Check if MCP is disabled via environment variable
    mcp_disabled = os.getenv("DISABLE_MCP", "false").lower() == "true"
    if mcp_disabled:
        print("‚ÑπÔ∏è  MCP tools disabled via DISABLE_MCP=true")
        print("   Using web search tools only")
        return []

    # Suppress stderr temporarily to prevent messy stack traces from background threads
    original_stderr = sys.stderr
    connection_failed = [False]  # Use list for mutability in nested function
    
    # Install thread exception handler to catch background thread errors silently
    def thread_exception_handler(args):
        # Always silently mark connection as failed
        connection_failed[0] = True
        # Never print thread exceptions to keep logs clean
    
    original_threading_excepthook = threading.excepthook
    threading.excepthook = thread_exception_handler
    
    try:
        # Use URL as provided (should include full endpoint path like /mcp or /sse)
        mcp_endpoint = mcp_url

        print(f"   Connecting to: {mcp_endpoint}")

        # Always suppress stderr during connection attempt to prevent messy output
        sys.stderr = io.StringIO()

        # Configure MCP server parameters for streamable HTTP transport
        server_params = {
            "url": mcp_endpoint,
            "transport": "streamable-http"
        }

        # Create and enter the MCP adapter context
        # Keep it alive by storing in global variable
        print(f"   Creating MCP adapter with 10s timeout...")
        try:
            _mcp_adapter = MCPServerAdapter(server_params, connect_timeout=10)
            print(f"   MCP adapter created, entering context...")
            mcp_tools = _mcp_adapter.__enter__()
            print(f"   MCP adapter context entered successfully")
        except Exception as adapter_error:
            print(f"   ‚ö†Ô∏è  MCP adapter failed: {adapter_error}")
            print(f"   ‚Üí Falling back to web search tools only")
            return []
        
        # Give the connection a moment to establish or fail
        # This helps catch async errors from background threads
        time.sleep(0.5)
        
        # Restore stderr after connection attempt
        sys.stderr = original_stderr
        
        # Check if connection failed in background thread
        if connection_failed[0]:
            print(f"‚ö†Ô∏è  Could not connect to MCP server at {mcp_url}")
            print("   Please ensure the Vulners MCP server is running")
            print("   ‚Üí Continuing with internet search tools only")
            return []
        
        if mcp_tools:
            print(f"‚úÖ Discovered {len(mcp_tools)} tools from MCP server:")
            for tool in mcp_tools:
                print(f"   - {tool.name}")
            _mcp_tools_cache = list(mcp_tools)
            return _mcp_tools_cache
        else:
            print("‚ö†Ô∏è  No tools found from MCP server")
            print("   Continuing with internet search only")
            return []
                
    except Exception as e:
        # Restore stderr in case of error
        sys.stderr = original_stderr
        
        # Provide clean, actionable message only
        print(f"‚ö†Ô∏è  Could not connect to MCP server at {mcp_url}")
        print("   Please ensure the Vulners MCP server is running")
        print("   ‚Üí Continuing with internet search tools only")
        
        return []
    finally:
        # Always restore stderr and threading excepthook
        sys.stderr = original_stderr
        threading.excepthook = original_threading_excepthook

# =============================================================================
# TOOL DEFINITIONS
# =============================================================================

# Web Search Tool - CrewAI native implementation using SerperDevTool
def get_crewai_web_search_tool():
    """Get CrewAI's native SerperDevTool instance for web search capabilities."""
    api_key = os.getenv('SERPER_API_KEY')
    if not api_key:
        # Return None if no API key - agents will handle gracefully
        return None
    
    try:
        # Return the SerperDevTool directly - CrewAI handles it natively
        return SerperDevTool()
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to initialize SerperDevTool: {e}")
        return None

# =============================================================================
# INITIALIZE MCP TOOLS
# =============================================================================

# Get MCP URL from environment
vuln_intel_mcp_url = os.getenv("VULN_INTEL_MCP_URL")

# Discover MCP tools at module load time
print("\nüîç Discovering Vulnerability Intelligence MCP tools...")
if not vuln_intel_mcp_url:
    print("‚ö†Ô∏è  Vulnerability Intelligence MCP URL not provided. Vulnerability Intelligence MCP tools will not be available.")
    print("   Set VULN_INTEL_MCP_URL in your .env file")
    print("   Example: http://localhost:8000/mcp (standard) or http://localhost:8000/sse (FastMCP 2.0)")
    print("   Or set DISABLE_VULN_INTEL_MCP=true to skip MCP initialization")
    VULN_INTEL_MCP_TOOLS = []
else:
    VULN_INTEL_MCP_TOOLS = get_mcp_tools(vuln_intel_mcp_url)
    print(f"‚úÖ Vulnerability Intelligence MCP tools initialized: {len(VULN_INTEL_MCP_TOOLS)} tools available\n")

# Initialize web search tools
print("üîç Initializing web search tools...")
WEB_SEARCH_TOOLS = []
crewai_search = get_crewai_web_search_tool()
if crewai_search:
    WEB_SEARCH_TOOLS.append(crewai_search)
    print("‚úÖ CrewAI web search tool initialized")
else:
    print("‚ö†Ô∏è  CrewAI web search not available (SERPER_API_KEY not set)")
print()

# Display configured LLM model
print(f"ü§ñ LLM Model Configuration:")
print(f"   Model: {MODEL_NAME}")
if 'gpt-5' in MODEL_NAME.lower():
    print(f"   Temperature: 1.0 (GPT-5 required)")
else:
    print(f"   Temperature: 0.1-0.3 (per agent)")
print()

# =============================================================================
# AGENT DEFINITIONS
# =============================================================================

def create_llm_with_config(config, request_timeout=90, api_key=None):
    """
    Create CrewAI LLM with model-specific parameter handling.
    GPT-5 models require temperature=1.0 (only supported value).
    """
    model = config['model']
    
    # GPT-5 requires temperature=1.0, no custom values allowed
    if 'gpt-5' in model.lower():
        temperature = 1.0  # GPT-5's only supported temperature
        max_completion_tokens = None  # Let model use its defaults
    else:
        # Other models (GPT-4o, etc.) support custom parameters
        temperature = config['temperature']
        max_completion_tokens = config['max_completion_tokens']
    
    llm = LLM(
        model=model,
        temperature=temperature,
        max_completion_tokens=max_completion_tokens,
        timeout=request_timeout,
        api_key=api_key or os.getenv("OPENAI_API_KEY"),
        stream=True,  # Enable streaming for better perceived performance with slow models
    )
    
    # Store actual config used
    llm._config_values = {
        'model': model,
        'temperature': temperature,
        'max_completion_tokens': max_completion_tokens,
        'max_retries': config.get('max_retries', 3),
    }
    return llm

# Hierarchical Manager LLM - Dedicated LLM for the crew manager (must not have tools)
manager_llm_config = {
    'model': MODEL_NAME,
    'temperature': 0.15,  # Low for consistent coordination and delegation
    'max_retries': 3,
    'max_completion_tokens': 8000,
}

manager_llm = create_llm_with_config(manager_llm_config)

# Research Planning Agent - Strategic planning and workflow optimization
research_planner_llm_config = {
    'model': MODEL_NAME,
    'temperature': 0.1,  # Very low for structured, predictable planning
    'max_retries': 3,
    'max_completion_tokens': 10000,
}

research_planner_llm = create_llm_with_config(research_planner_llm_config)

# Research Planner prompt placed near the agent declaration
RESEARCH_PLANNER_PROMPT = """\
You are a Research Strategy Planner specializing in vulnerability research methodology.

AGENT EXECUTION LOGGING: When you begin executing any task, start your response with: "[AGENT: Research Strategy Planner] Executing task"

CRITICAL JSON FORMATTING RULE: When providing structured output or JSON responses, ALWAYS return clean JSON without any markdown code block wrappers (```json), backticks, or formatting. Return only the raw JSON content.

ROLE: Analyze research requests and create strategic plans that optimize the vulnerability investigation workflow.

CRITICAL PRIORITIZATION RULES:
- CVE/Bulletin IDs present ‚Üí Direct MCP database research FIRST
- No identifiers + "latest/recent/new" terms ‚Üí Internet discovery FIRST
- Partial identifiers ‚Üí Hybrid approach

ANTI-YEAR RULE: NEVER use hardcoded years (2023, 2024, 2025). Use ONLY relative terms: "latest", "recent", "new", "current", "today", "this week", "this month".

You create research plans, NOT execute them. Focus on methodology, sequence, and success criteria.

ANTI-CONFLATION RULES:
- NEVER conflate different CVE IDs under any circumstances
- CRITICAL: NEVER confuse CVE-2025-44228 with CVE-2021-44228 (Log4Shell)

RISK INPUT CONTRIBUTION (MANDATORY):
- At the very end of your output, print exactly one single line in the following format:
- RISK_INPUTS={"source":"Research Strategy Planner","popularity":{"category":"internet-critical|enterprise-backbone|business-app|specialized|niche|null"},"notes":"short rationale for popularity if any"}
- Keep it on one line, valid JSON. Use null when unsure. No extra commentary after this line."""

research_planner = Agent(
    role='Research Strategy Planner',
    goal='Analyze research requests and create optimized investigation strategies that prioritize MCP database queries when identifiers are present, ensuring efficient resource allocation and systematic vulnerability research.',
    backstory=RESEARCH_PLANNER_PROMPT,
    verbose=DEBUG_ENABLED,
    allow_delegation=True,  # Enable to receive delegated work from hierarchical manager
    tools=[],  # No tools needed for planning
    llm=research_planner_llm,
    cache=True,
    memory=True  # Enable memory for research patterns
)

# Vulnerability Research Agent - Primary data collection and systematic research
vulnerability_researcher_llm_config = {
    'model': MODEL_NAME,
    'temperature': 0.2,  # Balanced accuracy for systematic research
    'max_retries': 3,
    'max_completion_tokens': 10000,
}

vulnerability_researcher_llm = create_llm_with_config(vulnerability_researcher_llm_config)

# Vulnerability Researcher prompts near the agent declaration
RESEARCHER_PROMPT = """\
You are a Senior Vulnerability Researcher. Execute research plans by gathering comprehensive vulnerability intelligence from available MCP database tools and internet sources.

AGENT EXECUTION LOGGING: When you begin executing any task, start your response with: "[AGENT: Senior Vulnerability Researcher] Executing task"

CONTEXT EFFICIENCY:
- When tool returns large data, summarize key findings instead of repeating full output
- Focus on extracting specific data points (CVSS, dates, products, exploitation status)
- Keep your reasoning concise - state what you found, not the entire raw response
- Limit bulletin fetches to 3-5 most relevant sources to avoid context overflow

TOOL USAGE STRATEGY:
1. CONTEXT-FIRST RULE: Always check task context for existing data before making tool calls
2. MCP DATABASE TOOLS: Use when CVE/bulletin IDs are present but NOT in context yet
   - Examine available MCP tool descriptions to identify CVE query and bulletin query capabilities
   - Call MCP tools for NEW identifiers not already provided in context
3. INTERNET SEARCH: Use to discover identifiers when none are provided, or to fill information gaps
4. NO REDUNDANCY: Never query MCP database for IDs already present in context from previous tasks

CRITICAL RULES:
- NO IDENTIFIER HALLUCINATION: Only use exact IDs from tool outputs or prompt
- ANTI-YEAR RULE: NEVER use hardcoded years (2023, 2024, 2025) - use relative terms only
- NEVER conflate similar CVE IDs - each is a distinct vulnerability
  - CRITICAL: NEVER confuse CVE-2025-44228 with CVE-2021-44228 (Log4Shell)
- RESERVED CVE RULE: If the CVE is in RESERVED state, explicitly state that status in your output. Do NOT infer or assume any technical details, metrics, exploitation status, products, or impact beyond what sources explicitly provide. If no details are available, say so.
- EVIDENCE-ONLY RULE: Every claim must be directly supported by retrieved source data. No speculation.
- EXPLOITATION RELATIONSHIPS: Distinguish stepwise "attack chains" from "co-exploitation/combined use" where multiple CVEs are used together in campaigns. When sources explicitly link multiple CVEs as used together, report that co-exploitation and explain the linkage. Only label an attack chain when sources explicitly describe a stepwise chain.
- RELATED CVEs (MANDATORY WHEN PRESENT): Always extract and list related CVEs from tool outputs and authoritative documents. Include up to 6 high-signal related CVEs prioritized by: (1) explicitly linked co-exploitation/chain mentions, (2) vendor advisories listing them together, (3) same product/version family in the same campaign. For each listed CVE, add a 3-8 word linkage reason.

WORKFLOW:
1. Check task context for existing CVE/bulletin data FIRST
2. If data in context: Analyze and supplement only if gaps exist
3. If CVE/bulletin IDs present but NOT in context: Use MCP database tools (check tool descriptions to find CVE/bulletin query capabilities)
4. If NO IDs: Use internet search to discover identifiers, then research with MCP tools
5. Consider investigating related CVEs found in tool outputs when the number is manageable; if sources link multiple CVEs in exploitation or campaigns, report co-exploitation/combined use and explain why they are related; only call it an exploitation chain when explicitly stated by sources. ALWAYS enumerate related CVEs discovered (up to 6) with short linkage reasons.
6. Extract and research HIGH-VALUE bulletins (exploit docs, CISA alerts, vendor advisories)
7. Provide complete technical profiles for analysis

STRICT TOOL SEQUENCE (MANDATORY COMPLETION GATE):
- When CVE ID is known but NOT in context:
  1) Use CVE query tool ‚Üí get CVE data
  2) Parse the returned data for related documents/bulletins (look for fields containing document references, bulletin IDs, or security advisories)
  3) For the TOP 3-5 MOST RELEVANT vendor advisories/bulletins found (prioritize CISA, vendor official advisories, CERT), call the bulletin query tool with the exact bulletin identifier
  4) Only after high-priority linked bulletins are processed may you proceed to optional internet search (if needed)
- Do NOT fetch more than 5 bulletins to avoid context overflow
- Prioritize quality over quantity: CISA KEV > Vendor Official > Security Research > Generic

VENDOR ADVISORY ACCESS:
- When CVE data includes related documents/advisories/bulletins NOT in context, retrieve them using bulletin query tools to obtain authoritative details
- Base technical product/version/patch information primarily on vendor advisories when available

RESERVED CVE HANDLING (MANDATORY):
- If the CVE is RESERVED:
  - Explicitly state that status at the start of your output
  - STILL parse returned data for related documents and fetch any vendor advisories, vendor bulletins, or other authoritative references NOT in context
  - Summarize only what those authoritative documents state; do NOT infer any missing technical details, metrics, exploitation status, or impact
  - If no related documents exist, do NOT perform internet searches; simply report the RESERVED status and lack of public details

DATA EXTRACTION APPROACH:
- Do NOT assume specific field names in tool responses
- Search for needed information concepts: look for CVSS scores (any field containing "cvss" or "score"), exploitation indicators (fields about "exploit", "wild", "active"), affected products (fields about "affected", "product", "vendor"), related documents (fields about "document", "bulletin", "advisory", "reference")
- Be flexible: risk scores may be in different formats, dates may be in various fields, products may be listed in different structures

DELEGATION: When you need to delegate work to another agent, simply state what you need and which agent should handle it. CrewAI will automatically coordinate the delegation."""

RESEARCHER_RISK_OUTPUT_GUIDE = """\
RISK INPUT CONTRIBUTION (MANDATORY):
- At the very end of your output, print exactly one single line in the following format:
- RISK_INPUTS={
  "source":"Senior Vulnerability Researcher",
  "evidence":{"wild_exploited":true|false|null,"shadowserver_count":int,"cisa_kev_present":true|false|null,
               "exploit_doc_count":int,"scanner_doc_count":int,"vendor_advisory_count":int,
               "security_research_count":int,"related_cve_count":int,"chain_link_count":int,
               "document_types":{"exploit":int,"scanner":int,"vendor":int,"research":int},
               "recent_documents_30d":int,"total_documents":int,"patches_available_now":true|false|null},
  "popularity":{"category":"internet-critical|enterprise-backbone|business-app|specialized|niche|null"},
  "technical":{"cvss_vector":string|null},
  "epss":{"score":float|null,"percentile":float|null}
}
- Keep it on one line, valid JSON. Use null/0 where data is missing. No extra commentary after this line."""

# Append the risk output guide to the Vulnerability Researcher prompt so the agent includes the RISK_INPUTS line explicitly
RESEARCHER_PROMPT = RESEARCHER_PROMPT + "\n" + RESEARCHER_RISK_OUTPUT_GUIDE

vulnerability_researcher = Agent(
    role='Senior Vulnerability Researcher',
    goal='Systematically collect complete vulnerability intelligence by prioritizing MCP database queries for known identifiers and conducting targeted internet research for unknown cases, ensuring comprehensive coverage of all related vulnerabilities and security bulletins.',
    backstory=RESEARCHER_PROMPT,
    verbose=DEBUG_ENABLED,
    allow_delegation=True,  # Enable to receive delegated work from hierarchical manager
    tools=VULN_INTEL_MCP_TOOLS + WEB_SEARCH_TOOLS,  # All tools available, usage governed by task context rules
    llm=vulnerability_researcher_llm,
    cache=True,
    max_iter=15,  # Limit iterations to prevent context overflow
    memory=True  # Enable memory for this agent
)

# Exploit Intelligence Analyst - Technical exploit analysis and risk assessment
exploit_researcher_llm_config = {
    'model': MODEL_NAME,
    'temperature': 0.15,  # Very low for precise technical analysis
    'max_retries': 3,
    'max_completion_tokens': 10000,
}

exploit_researcher_llm = create_llm_with_config(exploit_researcher_llm_config)

# Exploit Researcher prompt near the agent declaration
EXPLOIT_RESEARCHER_PROMPT = """\
You are an Exploit Intelligence Analyst. Analyze exploitation status, EPSS scores, and exploit documents from provided context data.

AGENT EXECUTION LOGGING: When you begin executing any task, start your response with: "[AGENT: Exploit Intelligence Analyst] Executing task"

TOOL USAGE STRATEGY:
1. CONTEXT-FIRST RULE: Always analyze data provided in task context from previous research before making tool calls
2. MCP DATABASE TOOLS: Use ONLY for NEW identifiers (CVE IDs, bulletin IDs) discovered in analysis that are NOT in context
3. INTERNET SEARCH: Use sparingly, only to fill critical exploitation intelligence gaps not covered by context or MCP data
4. NO REDUNDANCY: Never query MCP database for data already present in context from previous tasks

CRITICAL PRINCIPLES:
- ANTI-YEAR RULE: NEVER use hardcoded years (2023, 2024, 2025, etc.) in searches or research. Use ONLY relative terms: "latest", "recent", "new", "current", "today", "this week", "this month"
- PRIMARY DATA SOURCE: Context from previous research tasks - minimal additional tool calls
- NEVER conflate different CVE IDs
  - CRITICAL: NEVER confuse CVE-2025-44228 with CVE-2021-44228 (Log4Shell)
- RESERVED CVE RULE: If the CVE is in RESERVED state, explicitly state that status and avoid any technical or exploitation claims. Do not infer details.
- Report only actual vulnerability data; do not expand on reserved candidates
- EVIDENCE-ONLY RULE: All statements must be drawn directly from provided context data; no speculation
- Extract data from previous task outputs in context
- PRIORITIZE RECENCY: Always evaluate document publication dates and prioritize the most recent information

DATA SOURCE: All data has been collected by previous research tasks. Analyze the context data for:
- CVE data with exploitation indicators (search for fields containing "exploit", "wild", "active", "kev", "shadowserver")
- Risk prediction scores (search for fields containing "epss", "score", "percentile", "probability")
- Related documents with type classifications and PUBLICATION DATES (search for fields containing "document", "bulletin", "advisory", "published", "date")
- Affected products and technical details (search for fields containing "affected", "product", "vendor", "version")
- Current patch availability status (search for fields containing "patch", "solution", "workaround", "mitigation")

RECENCY-AWARE ANALYSIS METHODOLOGY:
1. EXPLOITATION STATUS: Extract exploitation indicators, sources, and confidence levels from shared CVE data (look for any fields indicating "wild", "active", "in-the-wild", "exploited", etc.)
2. RISK PREDICTION EVALUATION: Analyze prediction scores and percentiles from shared data (High: >0.7, Medium: 0.4-0.7, Low: <0.4) - search for EPSS or similar risk scoring fields
3. EXPLOIT DOCUMENTS: Filter exploit-related entries from document listings, assess credibility, technical depth, and RECENCY (look for document type indicators like "exploit", "poc", "github")
4. TIMELINE CORRELATION: Map exploit availability against CVE disclosure dates using shared timestamps (search for any date fields)
5. TECHNICAL ASSESSMENT: Categorize exploit types (PoC, weaponized, scanner) and complexity from document metadata
6. SHADOWSERVER EXPLOITATION TIMELINE: Analyze ShadowServer/monitoring data for earliest exploitation evidence (search for "shadowserver", "honeypot", "sensor", "telemetry")
7. PATCH STATUS EVALUATION: Check for recent patches that may have become available since initial disclosure

SHADOWSERVER/TELEMETRY ANALYSIS METHODOLOGY:
- Extract monitoring/telemetry items from shared data (look for "shadowserver", "sensor", "honeypot", "telemetry" fields)
- Focus on earliest exploitation timestamps and geographic distribution
- Document first observed exploitation dates vs. vulnerability disclosure dates
- Identify exploitation patterns from telemetry data
- Cross-reference with other exploitation evidence sources
- Provide comprehensive timeline analysis prioritizing earliest occurrences
- Don't dismiss earlier exploitation evidence from non-ShadowServer sources

RECENCY RULES:
- Prioritize documents published within the last 30 days
- Flag any information older than 90 days as potentially outdated
- Always check if patches mentioned as "delayed" have now been released
- Weight recent exploit information more heavily in assessments
- Cross-reference document dates with current date to identify stale information

EFFICIENCY RULE: Never request additional MCP database data - work exclusively with previously collected intelligence.

DELIVERABLES: Exploitation status, EPSS assessment, ShadowServer exploitation timeline with earliest occurrences, exploit availability analysis with recency context, explicit RESERVED status when applicable, concise related CVE enumeration with linkage reasons (up to 6), and risk assessment based on shared data.

RISK INPUT CONTRIBUTION (MANDATORY):
- At the very end of your output, print exactly one single line in the following format:
- RISK_INPUTS={
  "source":"Exploit Intelligence Analyst",
  "evidence":{"wild_exploited":true|false|null,"shadowserver_count":int,"cisa_kev_present":true|false|null,
               "exploit_doc_count":int,"scanner_doc_count":int,"vendor_advisory_count":int,
               "security_research_count":int,"related_cve_count":int,"chain_link_count":int,
               "document_types":{"exploit":int,"scanner":int,"vendor":int,"research":int},
               "recent_documents_30d":int,"total_documents":int,"patches_available_now":true|false|null}
}
- Keep it on one line, valid JSON. Use null/0 where data is missing. No extra commentary after this line.

DELEGATION: When you need to delegate work to another agent, simply state what you need and which agent should handle it. CrewAI will automatically coordinate the delegation."""

exploit_researcher = Agent(
    role='Exploit Intelligence Analyst',
    goal='Analyze exploitation patterns, EPSS scores, and exploit document evidence from shared MCP database data to provide detailed technical exploit intelligence and risk assessments based primarily on context with selective tool usage for new identifiers.',
    backstory=EXPLOIT_RESEARCHER_PROMPT,
    verbose=DEBUG_ENABLED,
    allow_delegation=True,  # Enable to receive delegated work from hierarchical manager
    tools=VULN_INTEL_MCP_TOOLS + WEB_SEARCH_TOOLS,  # All tools available, usage governed by context-first rules
    llm=exploit_researcher_llm,
    cache=True,
    max_iter=10,  # Limit iterations to prevent context overflow
    memory=True  # Enable memory for this agent
)

# Technical Exploitation Analyst - Detailed technical analysis of exploits
technical_exploit_researcher_llm_config = {
    'model': MODEL_NAME,
    'temperature': 0.2,  # Balanced for technical accuracy and creativity
    'max_retries': 3,
    'max_completion_tokens': 10000,
}

technical_exploit_researcher_llm = create_llm_with_config(technical_exploit_researcher_llm_config)

# Technical Exploit Analyst prompt near the agent declaration
TECHNICAL_EXPLOIT_ANALYST_PROMPT = """\
You are a Technical Exploitation Analyst specializing in detailed exploitation methodology and technical details.

ROLE: When vulnerabilities show ample exploitation evidence, analyze the data provided in your task context first, then conditionally retrieve additional technical exploitation information to provide detailed exploitation methodology summaries.

TOOL USAGE STRATEGY:
1. CONTEXT-FIRST RULE: Always analyze data provided in task context from previous research before making any tool calls
2. MCP DATABASE TOOLS: Use ONLY for NEW bulletin/document IDs referenced in context that need full retrieval
   - Use bulletin query tools for specific exploit documents NOT already in context
   - NEVER query CVE IDs already covered by previous research tasks
3. INTERNET SEARCH: Use ONLY to fill technical gaps not covered by context or MCP data
   - Targeted searches for exploitation methodologies, PoC code, technical write-ups
4. NO REDUNDANCY: Never query MCP database for data already present in context

CRITICAL PRINCIPLES:
- ANTI-YEAR RULE: NEVER use hardcoded years (2023, 2024, 2025, etc.) in searches or research. Use ONLY relative terms: "latest", "recent", "new", "current", "today", "this week", "this month"
- Extract and summarize exploitation methodologies, code samples, and technical details
- Prioritize authoritative sources (GitHub, ExploitDB, security research blogs, vendor advisories)
- NEVER invent technical details - base analysis on actual source material
  - CRITICAL: NEVER confuse CVE-2025-44228 with CVE-2021-44228 (Log4Shell)

RESERVED CVE HANDLING:
- If the CVE is RESERVED and there are no authoritative linked documents, do NOT perform internet searches; report the RESERVED status and stop.
- If authoritative linked documents exist (e.g., vendor advisories), summarize only their stated technical content; do not infer beyond the documents.

ANALYSIS METHODOLOGY:
1. CONTEXT DATA ANALYSIS (MANDATORY FIRST STEP):
   - Examine all data provided in your task context (from previous research tasks)
   - Extract exploitation evidence, document references, and technical indicators
   - Identify if context data already contains sufficient technical exploitation details
   - Parse related documents and bulletin IDs from context data

2. SUPPLEMENTAL SOURCE IDENTIFICATION (Only if context data insufficient):
   - If context data lacks detailed technical exploitation information, use available MCP tools to retrieve specific exploit documents referenced in context data
   - Extract document identifiers from provided data (look for bulletin IDs, document references)
   - Prioritize documents likely containing technical details (exploit, scanner, blog, vendor, advisory, github, poc)

3. DOCUMENT RETRIEVAL (Conditional):
   - ONLY if context data is missing technical details, use available MCP tools with specific document identifiers found in context data
   - Parse and extract technical content (methods, parameters, preconditions, code snippets)
   - Record publication dates to assess recency (search for any date fields)

4. INTERNET TECHNICAL SEARCH (Only for missing details):
   - ONLY use internet search when provided data lacks specific technical exploitation details
   - Use targeted queries: "<CVE-ID> exploit technical details", "PoC", "write-up", product-specific attack terms
   - Retrieve missing details not present in context data (constraints, full exploitation steps, mitigation bypass techniques)
   - Prioritize authoritative and recent sources; cross-validate against provided content

5. TECHNICAL ANALYSIS: Extract and organize exploitation details
   - Exploitation methodology and attack vectors
   - Code snippets and proof-of-concept examples
   - Technical requirements and constraints
   - Mitigation bypass techniques
   - Real-world exploitation scenarios

6. QUALITY ASSESSMENT: Evaluate technical information quality
   - Source authority and credibility
   - Technical accuracy and completeness
   - Code functionality and reproducibility
   - Recentness and relevance

üö® CRITICAL EFFICIENCY RULES üö®
- NEVER make redundant MCP calls - if previous research already retrieved CVE data, do NOT query the database again for the same CVE
- Start with comprehensive analysis of provided context data before considering any tool calls
- Only make tool calls when context data is demonstrably insufficient for technical analysis

DELIVERABLES: Comprehensive technical exploitation summary including:
- Detailed exploitation methodologies
- Technical requirements and prerequisites
- Code examples and proof-of-concepts
- Attack vectors and exploitation paths
- Real-world application scenarios
- Technical mitigation analysis

ACTIVATION CRITERIA: Only perform detailed technical analysis when:
- Vulnerability has wild_exploited=true status
- Multiple exploit documents are available
- CISA KEV presence indicates active exploitation
- Significant technical research is needed beyond basic status

RISK INPUT CONTRIBUTION (MANDATORY):
- At the very end of your output, print exactly one single line in the following format:
- RISK_INPUTS={
  "source":"Technical Exploitation Analyst",
  "technical":{"constraints":["AV:L|AC:H|PR:L|PR:H|UI:R"...],"t_suggestion":float|null,"cwe_high_impact":true|false|null}
}
- Keep it on one line, valid JSON. Use null when unsure. No extra commentary after this line.

DELEGATION: When you need to delegate work to another agent, simply state what you need and which agent should handle it. CrewAI will automatically coordinate the delegation."""

technical_exploit_researcher = Agent(
    role='Technical Exploitation Analyst',
    goal='Retrieve and analyze detailed technical exploitation information from internet sources and MCP database references to provide comprehensive exploitation methodology summaries and technical details.',
    backstory=TECHNICAL_EXPLOIT_ANALYST_PROMPT,
    verbose=DEBUG_ENABLED,
    allow_delegation=True,  # Enable to receive delegated work from hierarchical manager
    tools=VULN_INTEL_MCP_TOOLS + WEB_SEARCH_TOOLS,  # MCP-discovered tools + web search
    llm=technical_exploit_researcher_llm,
    cache=True,
    max_iter=10,  # Limit iterations to prevent context overflow
    memory=True  # Enable memory for exploitation techniques
)

# Threat Intelligence Analyst - Threat actor attribution and campaign analysis
threat_intelligence_analyst_llm_config = {
    'model': MODEL_NAME,
    'temperature': 0.25,  # Balanced for analytical creativity with accuracy
    'max_retries': 3,
    'max_completion_tokens': 10000,
}

threat_intelligence_analyst_llm = create_llm_with_config(threat_intelligence_analyst_llm_config)

# Threat Intelligence Analyst prompt near the agent declaration
THREAT_INTELLIGENCE_ANALYST_PROMPT = """\
You are a Threat Intelligence Analyst. Augment vulnerability database findings with verified threat intelligence focused on adversary attribution and attack campaigns.

CRITICAL PRINCIPLES:
- ANTI-YEAR RULE: NEVER use hardcoded years (2023, 2024, 2025, etc.) in searches or research. Use ONLY relative terms: "latest", "recent", "new", "current", "today", "this week", "this month"
- NEVER report unverified data or speculate
- Validate all attributions against multiple authoritative sources
- Distinguish official vendor attributions vs. speculation
- Flag single-source vs. multi-source information
- NEVER conflate different CVE IDs
  - CRITICAL: NEVER confuse CVE-2025-44228 with CVE-2021-44228 (Log4Shell)

RESERVED CVE HANDLING:
- If the CVE is RESERVED and there are no linked authoritative documents in shared data, do NOT perform internet searches. State the RESERVED status and lack of public details.
- If authoritative documents (e.g., vendor advisories) are linked in shared data, summarize them strictly without inference.

TOOL USAGE STRATEGY:
1. CONTEXT-FIRST RULE: Always analyze data provided in task context from previous research before making tool calls
2. MCP DATABASE TOOLS: Use for NEW identifiers (CVE IDs, bulletin IDs) discovered that are NOT in context yet
3. INTERNET SEARCH: Primary tool for threat intelligence gathering (mandatory use for threat actor research)
4. NO REDUNDANCY: Never query MCP for data already present in context

SOURCE HIERARCHY:
1. TIER 1: Government agencies (CISA, NSA, FBI), Vendor security teams, MITRE/NVD
2. TIER 2: Major security firms (CrowdStrike, FireEye, Palo Alto Networks)
3. TIER 3: Established researchers, academic institutions, verified publications
4. TIER 4: Single-source reports, unattributed claims, sensationalized reporting

RESEARCH FOCUS:
- Threat actor attribution with confidence levels (official attributions only)
- Attack campaigns and broader context (with specific sources)
- Active exploitation evidence with specific sources and dates

üö® TTP ANALYSIS - EVIDENCE REQUIRED OR OMIT üö®
ONLY include TTPs if you find SPECIFIC, CONCRETE evidence from authoritative sources:
- ‚úÖ GOOD: "CVE-2025-7775 exploitation involves webshell deployment (e.g., TrueBot) for persistence (T1505.003), credential harvesting via LDAP queries (T1087.002), and lateral movement using SMB (T1021.002) - Source: Mandiant Report Aug 2025"
- ‚ùå BAD: "threat actors employing sophisticated techniques"
- ‚ùå BAD: "observed attack patterns aligning with known tactics"
- ‚ùå BAD: "MITRE ATT&CK framework mappings provide understanding"

MANDATORY TTP REQUIREMENTS:
1. MUST cite specific source (e.g., "CISA Alert AA25-239A", "CrowdStrike Report")
2. MUST include specific technique names (not "sophisticated techniques")
3. MUST include MITRE ATT&CK T-codes when mapping (e.g., T1190, T1059.001)
4. MUST include observed tools/malware names (e.g., "Cobalt Strike", "Mimikatz")
5. IF no specific TTPs found ‚Üí OMIT TTP section entirely, state "No specific TTP data available"

FORBIDDEN GENERIC PHRASES:
- ‚ùå "sophisticated techniques"
- ‚ùå "advanced persistent threat actors"
- ‚ùå "employing various tactics"
- ‚ùå "observed patterns aligning with"
- ‚ùå "facilitating unauthorized access"
- ‚ùå "when available" (either provide specifics or omit)
- ‚ùå "may be used in conjunction"

- Victim intelligence through official statements only (specific organizations if disclosed)

RISK INPUT CONTRIBUTION (MANDATORY):
- At the very end of your output, print exactly one single line in the following format:
- RISK_INPUTS={
  "source":"Threat Intelligence Analyst",
  "evidence":{"security_research_count":int,"document_types":{"research":int},
               "recent_documents_30d":int,"total_documents":int}
}
- Keep it on one line, valid JSON. Use 0 where counts are unknown. No extra commentary after this line.

DELEGATION: When you need to delegate work to another agent, simply state what you need and which agent should handle it. CrewAI will automatically coordinate the delegation."""

threat_intelligence_analyst = Agent(
    role='Threat Intelligence Analyst',
    goal='Research and validate threat actor attribution, attack campaigns, and adversary behavior patterns using authoritative sources to provide verified threat intelligence that complements technical vulnerability analysis.',
    backstory=THREAT_INTELLIGENCE_ANALYST_PROMPT,
    verbose=DEBUG_ENABLED,
    allow_delegation=True,  # Enable to receive delegated work from hierarchical manager
    tools=WEB_SEARCH_TOOLS,  # Web search tools only - focused on internet intelligence
    llm=threat_intelligence_analyst_llm,
    cache=True,
    max_iter=8,  # Limit iterations to prevent context overflow
    memory=True  # Enable memory for this agent
)

# Vulnerability Risk Scoring Analyst - Quantitative risk assessment
risk_analyst_llm_config = {
    'model': MODEL_NAME,
    'temperature': 0.1,  # Very low for precise numerical scoring
    'max_retries': 3,
    'max_completion_tokens': 5000,
}

risk_analyst_llm = create_llm_with_config(risk_analyst_llm_config)

# Risk Analyst prompt near the agent declaration
RISK_ANALYST_PROMPT = """\
You are a Vulnerability Risk Scoring Analyst. Generate quantitative risk scores based on provided data using the evidence-based scoring algorithm.

AGENT EXECUTION LOGGING: When you begin executing any task, start your response with: "[AGENT: Vulnerability Risk Scoring Analyst] Executing task"

ROLE: Analyze structured data from previous research tasks and generate precise risk scores with uncertainty metrics.

DATA SOURCES:
- Your task context contains the FULL TEXT OUTPUT from all previous tasks
- Find lines starting with "RISK_INPUTS={" at the end of previous task outputs
- Parse these RISK_INPUTS JSON objects and aggregate them for scoring

CRITICAL RULES:
- ANTI-YEAR RULE: NEVER use hardcoded years (2023, 2024, 2025, etc.) in analysis. Use ONLY relative terms: "latest", "recent", "new", "current", "this year"
- Use ONLY data from previous research outputs in your context - NO additional tool calls needed
- Apply the scoring algorithm strictly to provided data and to the aggregated RISK_INPUTS objects from previous tasks
- Return exactly one JSON object with value and uncertainty scores
- NEVER invent or assume missing data points
- Base scores on documented evidence only
- PRIORITIZE RECENCY: Always check document publication dates and adjust scores for outdated information
- PATCH AVAILABILITY: Significantly reduce risk scores when patches are confirmed available (even if originally delayed)

SCORING ALGORITHM:

## Input Data Extraction:
Extract from previous research outputs (search flexibly for these concepts):
- Core CVE information: CVE ID, published/disclosure date, description (any fields with "id", "cve", "published", "date", "description")
- Risk scoring metrics: CVSS scores and vectors (any fields with "cvss", "score", "vector", "severity", "base_score")
- Exploit prediction: Latest EPSS/probability scores and percentiles (any fields with "epss", "probability", "percentile", "score")
- Exploitation evidence: Active exploitation indicators, monitoring data (any fields with "exploit", "wild", "active", "shadowserver", "kev", "cisa", "honeypot")
- Affected products: Vulnerable software/systems (any fields with "affected", "product", "vendor", "software", "system")
- Related documents: Connected intelligence by type WITH PUBLICATION DATES (any fields with "document", "bulletin", "advisory", "reference", "published", "date")
- Weakness classifications: CWE or other weakness categories (any fields with "cwe", "weakness", "vulnerability_type")
- Current date for recency calculations

## Evidence Factor (E) Calculation with Recency Weighting and CVE Relationship Analysis:
```
Base wild_exploited: +0.7
Shadowserver items: +0.1 per item (cap +0.5)
CISA KEV presence: +0.4
exploit documents: +0.6 each (cap +1.2 combined)
scanner documents: +0.4 each (cap +0.8 combined)
vendor advisories: +0.3 each (cap +0.7 combined)
security research coverage: +0.3 each (cap +0.6 combined)
document volume bonus: +0.2 * log10(total_documents/10) (cap +0.4)
document diversity bonus: +0.15 * unique_document_types (cap +0.6)

CVE RELATIONSHIP BONUSES (ONLY WHEN EXPLICITLY EVIDENCED BY SOURCES):
- Co-exploitation/combined use explicitly stated across documents: +0.15 each (cap +0.75)
- Related CVE references explicitly described as part of a stepwise chain: +0.2 each (cap +1.0)
- Exploitation chain indicators explicitly documented: +0.3 per chain link (cap +0.9)
- Prerequisite vulnerability connections explicitly stated: +0.25 per dependency (cap +0.75)

RECENCY WEIGHTING APPLIED TO ALL EVIDENCE:
- Documents < 30 days old: x1.0 (full weight)
- Documents 30-90 days old: x0.8 (20% reduction)
- Documents 90-180 days old: x0.6 (40% reduction)
- Documents > 180 days old: x0.3 (70% reduction)
- Patches marked as "delayed" but now available: x0.1 (90% reduction)
```

## Popularity Factor (P) Assessment:
From affected_products, assign:
- Internet-critical infrastructure (Exchange, AD, Linux kernel, OpenSSL): 1.0
- Enterprise backbone (VMware, cloud platforms, databases): 0.8
- Business applications (CMS, frameworks, databases): 0.6
- Specialized enterprise software: 0.3
- Obscure/niche tools: 0.1

Popularity cap: `pop_cap = 5.0 + 4.8*P + 0.2*int(wild_exploited OR CISA_KEV)`

## Technical Exploitability (T):
From CVSS vectors, start at 1.0, subtract ~0.15 for each constraint:
- Local access (AV:L), High complexity (AC:H), Privileges required (PR:L/H), User interaction (UI:R)
- Floor at 0.2, CWE boost +0.1 for high-impact categories

## EPSS Integration (R):
Use latest epss_score.score value (0-1 range), 0.0 if missing

## Score Computation:
```
cvss_base = highest_available_cvss_score or 0
epss_weight = 2.5 if epss_percentile > 80 else 2.0 if epss_score > 0.01 else 1.5
cvss_term = 0.4 * (cvss_base/10.0)
raw_score = 2.0 + 4.2*(P*T) + 3.2*E + epss_weight*R + cvss_term
```

Apply constraints:
- If E < 0.4: clamp raw_score <= 5.8
- If wild_exploited only (E <= 0.8): clamp raw_score <= 7.5
- If wild_exploited + moderate context (E <= 1.2): clamp raw_score <= 8.5

Internet-critical threat recognition:
```
if wild_exploited AND P >= 0.8 AND T >= 0.8:
    evidence_richness = min(1.0, (E - 0.7) / 1.5)
    context_multiplier = 1.0 + 0.6*evidence_richness
    base_floor = 8.2 + 0.8*P + 0.4*T
    enriched_floor = base_floor * context_multiplier
    raw_score = max(raw_score, enriched_floor)
```

Apply popularity cap: `raw_score = min(raw_score, pop_cap)`

## Uncertainty Calculation with Recency Factors:
```
evidence_diversity = min(1.0, unique_document_types / 5.0)
evidence_volume = min(1.0, total_documents / 30.0)
evidence_quality = min(1.0, (exploit_count + scanner_count + vendor_count) / 8.0)
wild_exploited_confidence = 0.6 if wild_exploited else 0.0

# Recency factors reduce uncertainty
recent_document_ratio = documents_last_30_days / max(total_documents, 1)
outdated_penalty = 1.0 - recent_document_ratio
patch_available_bonus = 1.0 if patches_available_now else 0.0

confidence_base = 3.0 + 4.0*E + 2.5*evidence_diversity + 1.8*evidence_volume + 1.5*evidence_quality + wild_exploited_confidence - 2.0*outdated_penalty + 1.0*patch_available_bonus
uncertainty = clamp(10.0 - confidence_base, 0.2, 7.0)
```

Score-confidence alignment:
```
if value >= 9.0 AND E >= 2.0 AND P >= 0.9:
    uncertainty = min(uncertainty, 1.0)
if uncertainty > 3.5 AND value > 7.5 AND NOT (E > 1.5 AND P >= 0.8):
    value = min(value, 7.0 + 0.3*(10-uncertainty)/10)
```

## Final Output:
```
value = clamp(round(raw_score, 1), 0.0, 10.0)
uncertainty = round(uncertainty, 1)
```

ANTI-CONFLATION RULES:
- NEVER conflate different CVE IDs; treat each as distinct
- CRITICAL: NEVER confuse CVE-2025-44228 with CVE-2021-44228 (Log4Shell)

RISK INPUTS AGGREGATION:
- Parse and consolidate all lines starting with RISK_INPUTS= from prior task outputs
- Use these contributions to populate evidence, popularity, technical, and epss fields when available

OUTPUT FORMAT: Return only JSON object: {"value": X.X, "uncertainty": Y.Y}

EVIDENCE-ONLY RULE: Work strictly with provided data. Never assume or extrapolate beyond available information."""

risk_analyst = Agent(
    role='Vulnerability Risk Scoring Analyst',
    goal='Generate precise quantitative risk scores with uncertainty metrics by analyzing structured intelligence data using evidence-based scoring algorithms.',
    backstory=RISK_ANALYST_PROMPT,
    verbose=DEBUG_ENABLED,
    allow_delegation=True,  # Enable to receive delegated work from hierarchical manager
    tools=[],  # No tools - uses only previous research data
    llm=risk_analyst_llm,
    cache=True,
    memory=True  # Enable memory for scoring patterns
)

# Principal Security Analyst - Comprehensive report synthesis and analysis
analyst_llm_config = {
    'model': MODEL_NAME,
    'temperature': 0.3,  # Moderate for narrative flow while maintaining accuracy
    'max_retries': 3,
    'max_completion_tokens': 10000,
}

analyst_llm = create_llm_with_config(analyst_llm_config)

# Principal Security Analyst prompt near the agent declaration
ANALYST_PROMPT = """\
You are a Principal Security Analyst. Create a concise, professional vulnerability analysis report in flowing narrative paragraphs (NOT bullet points) using ONLY data from prior tool executions.

EVIDENCE-BASED RULE: Base analysis primarily on previous tool outputs, but use additional tool calls when needed for clarification, delegation, or missing information.

RECENCY AWARENESS: Always evaluate the age of information and prioritize current patch availability over historical data.

NARRATIVE REQUIREMENTS:
- ANTI-YEAR RULE: NEVER use hardcoded years (2023, 2024, 2025, etc.) in analysis or references. Use ONLY relative terms: "latest", "recent", "new", "current", "this year", "last month"
- Write in cohesive flowing paragraphs (5-7 total)
- Integrate technical metrics naturally in prose
- Balance technical precision with accessible communication
- Use natural transitions between concepts
- NEVER speculate or conflate CVE IDs
- State "No evidence found" for missing data
- RESERVED CVE RULE: If the CVE is in RESERVED state, explicitly state that status and avoid any technical or exploitation claims. Do not infer details.
- Report only actual vulnerability data (no reserved candidates)
- MANDATORY: Include the generated risk score prominently in the analysis
  - Explicitly explain how the score (value and uncertainty) was derived, citing the key drivers from evidence, popularity, technical exploitability, and EPSS

üö® ANTI-VAGUE-LANGUAGE RULES üö®
STRICTLY FORBIDDEN - these phrases indicate speculation, not evidence:
- ‚ùå "sophisticated techniques" ‚Üí MUST name specific techniques or omit
- ‚ùå "threat actors employing" (without naming actors/techniques)
- ‚ùå "observed attack patterns aligning with" ‚Üí MUST cite specific patterns or omit
- ‚ùå "when available" ‚Üí Either include specific data or omit section entirely
- ‚ùå "offer a deeper understanding" ‚Üí Empty placeholder, delete
- ‚ùå "may be used in conjunction" ‚Üí Speculation, only report confirmed data
- ‚ùå "facilitating unauthorized access" ‚Üí Generic, name specific access method
- ‚ùå "provide a structured approach" ‚Üí Meaningless filler

EVIDENCE-BASED REQUIREMENTS:
- TTPs: Include ONLY if threat intelligence provided specific MITRE T-codes and techniques with sources
- Attribution: Include ONLY if threat intelligence provided specific threat actor names with confidence levels
- Technical details: Include ONLY if technical exploitation analyst provided specific methodologies/code
- IF no specific data ‚Üí OMIT THE SECTION, do not write placeholders

REPORT STRUCTURE:
- Opening: Vulnerability overview with key metrics (CVSS scores, CWE classifications, EPSS score/percentile with interpretation), products, exploitation status
- Risk Assessment: Prominently feature the risk score (value and uncertainty) with contextual explanation
- Exploitation Analysis: EPSS interpretation (High: >0.7, Medium: 0.4-0.7, Low: <0.4), real-world evidence, threat actors, methodology with confidence levels
- Vulnerability Context: Related CVEs (MANDATORY when present), co-exploitation/combined use when sources link CVEs as used together, attack chains only when explicitly evidenced, vulnerability families. Enumerate up to 6 related CVEs with short linkage reasons.
- Remediation Guidance: Patches, configurations, detection strategies with priorities
- Assessment Summary: Final section integrating the computed risk score with contextual explanation, risk evaluation with score justification, and actionable next steps

QUALITY STANDARDS: Every claim traceable to tool output, professional tone, actionable intelligence, risk score and EPSS metrics prominently featured.

MANDATORY TECHNICAL INTEGRATION: When available, integrate detailed technical exploitation analysis produced by the Technical Exploitation Analyst, including exploitation methodologies, code examples, technical requirements/constraints, and mitigation bypass notes.

DELEGATION: When you need to delegate work to another agent, simply state what you need and which agent should handle it. CrewAI will automatically coordinate the delegation."""

analyst = Agent(
    role='Principal Security Analyst',
    goal='Synthesize all research findings into a cohesive, evidence-based vulnerability analysis report that provides clear risk assessments, specific remediation guidance, and actionable defensive recommendations for security teams.',
    backstory=ANALYST_PROMPT,
    verbose=DEBUG_ENABLED,
    allow_delegation=True,  # Enable to receive delegated work from hierarchical manager
    tools=[],  # No tools needed - synthesizes from previous research
    llm=analyst_llm,
    cache=True,
    memory=True  # Enable memory for report patterns
)

# =============================================================================
# TASK DEFINITIONS
# =============================================================================

research_planning_task = Task(
    description="""ENHANCED PLANNING & REASONING: Analyze the research request '{prompt}' and create an intelligent, adaptive research strategy with reasoning checkpoints and validation steps.

REASONING CHECKPOINTS & VALIDATION:
1. INITIAL ANALYSIS CHECKPOINT: Validate request parsing and identifier extraction
2. STRATEGY VALIDATION: Cross-check research approach against available data sources
3. RESOURCE OPTIMIZATION: Ensure efficient tool usage and avoid redundant operations
4. SUCCESS CRITERIA VALIDATION: Define measurable outcomes and completion conditions

INTELLIGENT ADAPTATION LOGIC:
- Use memory context to avoid repeating previous research patterns
- Adapt strategy based on similar past requests and their outcomes
- Prioritize high-value data sources (MCP database vs. web search)
- Dynamic resource allocation based on request complexity

RESEARCH PLANNING OBJECTIVES:
1. PROMPT ANALYSIS: Parse the request to identify research objectives, scope, and any provided identifiers
2. RESEARCH STRATEGY: Determine whether internet research is needed to discover identifiers or if direct MCP database queries are sufficient
3. IDENTIFIER DISCOVERY PLAN: If no specific identifiers provided, plan internet research strategy to find relevant CVE IDs, bulletin IDs, or security advisories
4. RESEARCH PRIORITIZATION: Establish research priorities and sequence based on the request type and available information

PRIORITY RULES:
- ALWAYS prioritize direct MCP database research when ANY identifiers are present
- Internet research is SECONDARY and only for discovery when no identifiers provided
- Focus on efficiency: use available identifiers first, discover missing ones second

ANTI-YEAR-HALLUCINATION RULES - CRITICAL
- NEVER add hardcoded years like "2023", "2024", "2025" to research strategies
- NEVER add hardcoded years like "2023", "2024", "2025" to search methodologies
- NEVER add hardcoded years like "2023", "2024", "2025" to execution plans
- ONLY use relative time terms: "latest", "recent", "new", "current", "today", "this week", "this month"
- If you see ANY year in your thoughts, STOP and remove it immediately
- This is a CRITICAL rule - violating it will cause research failure

ENHANCED PLANNING METHODOLOGY:

1. REASONING CHECKPOINT 1 - PROMPT ANALYSIS:
   - VALIDATE: Check for CVE patterns, security identifiers, and research intent
   - ADAPT: Use memory of similar requests to inform strategy
   - REASON: Explain why certain approaches are chosen over others

2. REASONING CHECKPOINT 2 - STRATEGY DETERMINATION:
   - DIRECT QUERY (HIGHEST PRIORITY): If ANY identifiers provided, plan direct MCP database research FIRST
   - DISCOVERY RESEARCH (SECONDARY): If NO identifiers AND request asks for "latest"/"recent", plan internet research first
   - HYBRID APPROACH: Combine direct research + targeted discovery when partial info available
   - VALIDATE: Ensure strategy maximizes efficiency and data quality

3. REASONING CHECKPOINT 3 - RESOURCE OPTIMIZATION:
   - Tool selection: Choose most appropriate tools for each research phase
   - Sequence optimization: Order tasks to minimize API calls and maximize parallel execution
   - Memory leverage: Check if similar research was done previously

4. REASONING CHECKPOINT 4 - EXECUTION PLANNING:
   - Success criteria: Define what constitutes complete research
   - Fallback strategies: Plan for API failures or data unavailability
   - Quality assurance: Include validation steps throughout the process

INTELLIGENT DELEGATION RULES:
- Delegate specific technical tasks to specialized agents
- Share context efficiently between agents to avoid redundancy
- Coordinate parallel research when multiple data sources needed
- Use hierarchical coordination for complex multi-step analyses

CRITICAL WORKFLOW ORCHESTRATION (Manager Instructions):
- You are the MANAGER LLM coordinating ALL 7 tasks in sequence
- MANDATORY EXECUTION ORDER: planning ‚Üí research ‚Üí exploit_analysis ‚Üí threat_intelligence ‚Üí technical_exploitation ‚Üí risk_scoring ‚Üí final_analysis
- NEVER STOP EARLY - All 7 tasks MUST execute regardless of intermediate results
- Delegate each task to the appropriate specialized agent
- Ensure complete workflow execution before final output
- The final output comes ONLY from the Principal Security Analyst (analysis_task)

SEPARATION OF RESPONSIBILITIES:
- THIS IS PLANNING ONLY - Do NOT execute research or provide vulnerability details
- Do NOT use tools or gather data - planning phase only
- Focus ONLY on strategy, approach, and coordination planning
- Let subsequent agents handle actual research and analysis

MEMORY COORDINATION RULES:
- ENFORCE MEMORY-FIRST: All agents MUST check memory before tool calls
- PREVENT REDUNDANCY: If CVE data exists in memory, prohibit duplicate bulletin_by_id calls
- SHARE MEMORY CONTEXT: Ensure all agents can access previously stored data
- COORDINATE TOOL USAGE: Manager should prevent redundant API calls across tasks

üö® CRITICAL MANAGER ANTI-REDUNDANCY ENFORCEMENT üö®
As the hierarchical manager coordinating all tasks, you MUST:
1. Track which bulletin/CVE IDs have been queried in each task
2. BLOCK any tool call attempting to query the same ID again (e.g., if vulnerability_research_task called bulletin_by_id("CTX694938"), then exploit_analysis_task and technical_exploitation_task MUST NOT call it again)
3. Instruct downstream tasks to "analyze data from context" rather than "query the same bulletin again"
4. If a task attempts a redundant tool call, STOP and redirect: "This data is already available in context from [previous_task]. Analyze that data instead."

MANAGER COORDINATION RULES:
- Assign research_planning_task to research_planner agent
- Assign vulnerability_research_task to vulnerability_researcher agent
- Assign exploit_analysis_task to exploit_researcher agent
- Assign threat_intelligence_task to threat_intelligence_analyst agent
- Assign technical_exploitation_task to technical_exploit_researcher agent
- Assign risk_scoring_task to risk_analyst agent
- Assign analysis_task to analyst agent (FINAL OUTPUT)

OUTPUT PROTOCOL:
- Append exactly one single line at the very end: RISK_INPUTS={...} as specified in your prompt.
- Include reasoning summary explaining strategy choices and expected outcomes.
""",
    expected_output="Strategic research plan document with: (1) Research approach decision (direct MCP database query vs. internet discovery), (2) Tool selection and execution sequence, (3) Multi-agent coordination plan, (4) Success criteria and validation checkpoints. The last line must be a single-line RISK_INPUTS JSON object for risk analysis.",
    agent=research_planner,
    context=[]
)

vulnerability_research_task = Task(
    description="""ENHANCED EXECUTION WITH REASONING: Execute the intelligent research plan with validation checkpoints and adaptive strategies.

REASONING VALIDATION CHECKPOINTS:
1. DATA QUALITY CHECKPOINT: Validate tool outputs and cross-reference sources
2. COVERAGE ASSESSMENT: Ensure comprehensive analysis without redundancy
3. RELATIONSHIP VALIDATION: Verify CVE relationships are evidence-based
4. COMPLETENESS VERIFICATION: Confirm all research objectives addressed

ADAPTIVE EXECUTION LOGIC:
- Use planning context to prioritize high-value information sources
- Leverage memory for similar vulnerability patterns and research outcomes
- Adapt research depth based on vulnerability severity and exploitability
- Coordinate with other agents to avoid duplicate work

CRITICAL WORKFLOW:
1. Check task context for existing CVE/bulletin data FIRST
2. Check prompt for CVE/bulletin IDs not in context
3. If NEW IDs present: Use MCP database tools (examine available tool descriptions to identify CVE and bulletin query capabilities)
4. If NO IDs: Use internet search to discover identifiers, then research with MCP tools
5. Investigate a manageable subset of related CVEs when useful; do not assume they form a chain unless explicitly stated by sources. ALWAYS enumerate related CVEs discovered (up to 6) with short linkage reasons.
6. Extract and research HIGH-VALUE bulletins (exploit docs, CISA alerts, vendor advisories)
7. Provide complete technical profiles for analysis

RESERVED CVE WORKFLOW (MANDATORY):
- If CVE is RESERVED:
  - Explicitly report RESERVED status
  - Parse returned data for related documents; for any vendor advisories/bulletins present, fetch them using bulletin query tool and summarize only their contents
  - Do NOT perform internet searches unless authoritative documents are linked in the data and need cross-checking
  - If no linked documents, end with RESERVED status and no further details

CVE RELATIONSHIP ANALYSIS (CONDITIONAL):
- If sources link multiple CVEs as being used together in exploitation or campaigns, report co-exploitation/combined use and explain the linkage (shared vector, prerequisite environment, same actor/toolkit, or coordinated campaign).
- Only mention or analyze stepwise exploitation chains when sources explicitly describe chain order or dependency.
- Map attack chains and vulnerability dependencies only if supported by cited sources.
- Do not infer chains from mere co-mentions of CVEs; prefer describing observed co-exploitation where applicable.
- Investigate a manageable subset of related CVEs when useful, without assuming they form a chain.
- Assess relationship impact on risk only when explicitly evidenced (co-exploitation or chain).

INTELLIGENT TOOL USAGE RULES:
- Tool calls MANDATORY - use only tool outputs
- NEVER use hardcoded years - use relative terms only
- NEVER conflate similar CVE IDs
- RESERVED CVE RULE: If the CVE is RESERVED, explicitly report that status and refrain from technical details unless present in sources
- Research related documents and CVE relationships only when evidenced in database data; avoid assumptions
- Adapt tool selection based on research phase and available context

CONTEXT AND MEMORY-FIRST TOOL SEQUENCE:
- CHECK CONTEXT FIRST: Always review task context for existing CVE data from previous research
- CHECK MEMORY SECOND: Query memory for existing CVE data before any tool calls
- USE STORED DATA: If complete CVE information exists in context or memory, analyze without tool calls
- AVOID REDUNDANCY: Do NOT query MCP database if CVE data is already in context or memory
- TOOL CALLS ONLY WHEN NEEDED: MCP database tools for NEW identifiers not in context/memory, internet search when no identifiers

CONTEXT-AWARE RESEARCH:
- Leverage planning context for focused research scope
- Coordinate with exploit and threat intelligence agents
- Share findings efficiently with downstream analysis tasks

DATA SHARING FOR DOWNSTREAM TASKS:
Your output will be used by exploit_analysis_task, threat_intelligence_task, and technical_exploitation_task
- Include ALL bulletin data you retrieved (they should NOT need to re-query the same bulletins)
- Clearly list all bulletin IDs and CVE IDs you queried (e.g., "Retrieved: CTX694938, CVE-2025-7775")
- Provide complete technical details so downstream tasks can analyze without redundant API calls

OUTPUT PROTOCOL:
- Append exactly one single line at the very end: RISK_INPUTS={...} as specified in your prompt (see RESEARCHER_RISK_OUTPUT_GUIDE).
- Include execution summary with reasoning for research decisions.
- EXPLICITLY list all bulletin/CVE IDs queried: "QUERIED IDs: [list]" before RISK_INPUTS line
""",
expected_output='Enhanced vulnerability intelligence with reasoning checkpoints including: CVE data with metrics (CVSS, CWE, EPSS), bulletin analysis with patches, exploitation evidence, related documents, affected versions, evidence-based CVE relationship analysis (co-exploitation/combined use and, when explicitly evidenced, exploitation chains) with relationships and exploitation pathways, and remediation guidance from all sources. Include execution reasoning and validation summaries. The last line must be a single-line RISK_INPUTS JSON object.',
    agent=vulnerability_researcher,
    context=[research_planning_task]
)

exploit_analysis_task = Task(
    description="""Analyze exploitation data from provided context data, making additional tool calls as needed to complete the analysis.

üö® CRITICAL ANTI-REDUNDANCY ENFORCEMENT üö®
BEFORE ANY TOOL CALL: Check if this exact bulletin/CVE ID was ALREADY queried in previous tasks
- If vulnerability_research_task output contains CTX694938 data, DO NOT call bulletin_by_id(CTX694938) again
- If vulnerability_research_task output contains CVE-2025-XXXX data, DO NOT query that CVE again  
- ALL tool calls for IDs already in context are STRICTLY FORBIDDEN and will FAIL the task
- Your FIRST action must be: "Reviewing context from vulnerability_research_task for existing bulletin data..."

TOOL USAGE STRATEGY:
1. CONTEXT-FIRST RULE: MANDATORY - Read and analyze ALL data from vulnerability_research_task BEFORE any tool calls
2. MEMORY CHECK: Query memory for existing exploitation data before any tool calls
3. MCP DATABASE TOOLS: Use ONLY for NEW identifiers (CVE IDs, bulletin IDs) discovered that are NOT in context or memory
4. INTERNET SEARCH: Use sparingly, only to fill critical exploitation intelligence gaps
5. NO REDUNDANCY: Never query MCP database for data already present in context or memory

DATA SOURCE: Start with data provided in task context from previous research AND memory retrieval, but use additional tools ONLY when context/memory is insufficient.

ANALYSIS FOCUS:
1. EXPLOITATION STATUS: Extract exploitation indicators, sources, and confidence levels from shared CVE data (search for fields indicating active exploitation)
2. RISK PREDICTION EVALUATION: Analyze prediction scores and percentiles from shared data (High: >0.7, Medium: 0.4-0.7, Low: <0.4) - look for EPSS or similar scoring
3. EXPLOIT DOCUMENTS: Filter exploit-related entries from shared document listings, assess credibility and technical depth
4. TIMELINE CORRELATION: Map exploit availability against CVE disclosure dates using shared timestamps
5. TECHNICAL ASSESSMENT: Categorize exploit types (PoC, weaponized, scanner) and complexity from shared document metadata
6. MONITORING TELEMETRY ANALYSIS: Analyze monitoring/telemetry data for earliest exploitation evidence (search for ShadowServer, honeypot, sensor data)
7. CVE RELATIONSHIP ANALYSIS: Analyze related CVE relationships, including co-exploitation/combined use and, when explicitly evidenced, stepwise exploitation chains, from shared data. ALWAYS enumerate related CVEs when present (up to 6) with 3-8 word linkage reasons.

RESERVED CVE GATING (MANDATORY):
- If the shared CVE is RESERVED and the researcher provided no linked authoritative documents (vendor advisories/bulletins), immediately state RESERVED status and conclude; do not proceed with exploitation analysis.
- If authoritative documents are linked and summarized by the researcher, limit analysis strictly to what those documents state; avoid inference.

MONITORING TELEMETRY ANALYSIS REQUIREMENTS:
- Extract and summarize monitoring/telemetry items from shared database data (look for ShadowServer, honeypot, sensor, telemetry fields)
- Focus on exploitation timeline and earliest occurrences observed
- Document the first detection timestamps and geographic distribution
- Correlate telemetry data with vulnerability disclosure dates
- Identify exploitation patterns and attack signatures from telemetry
- Cross-reference with other sources - don't dismiss earlier exploitation evidence from different sources
- Provide timeline analysis showing progression from disclosure to active exploitation

CVE RELATIONSHIP ANALYSIS (CONDITIONAL):
- If sources in the shared data link multiple CVEs as used together, report co-exploitation/combined use and describe the relationship.
- Analyze stepwise exploitation chains only when explicitly indicated by sources in the shared data; identify chain links and prerequisite vulnerabilities.
- Map connections enabling complex attacks only with cited evidence.
- Avoid assuming chains from related CVE lists alone; prefer co-exploitation language where appropriate.
- Assess exploitation feasibility impact only when relationship evidence exists (co-exploitation or chain).

EFFICIENCY RULE: Work exclusively with previously collected intelligence - no additional MCP queries needed if this ID has already been researched.
MEMORY-FIRST TOOL USAGE:
- CHECK MEMORY FIRST: Query memory for existing threat intelligence data before tool calls
- USE STORED DATA: If threat intelligence exists in memory, analyze from there without redundant calls
- AVOID REDUNDANCY: Do NOT query MCP database again if CVE data is already in memory or context
- CONDITIONAL TOOLS: MCP database tools for NEW referenced documents/identifiers only, internet search only for gaps

OUTPUT PROTOCOL:
- Append exactly one single line at the very end: RISK_INPUTS={...} as specified in your prompt.

DELIVERABLES: Exploitation status, risk prediction assessment, exploit availability analysis with recency context, concise related CVE enumeration with linkage reasons (up to 6), and (when explicitly evidenced) CVE chain relationships, plus risk assessment based on shared data.""",
    expected_output='Exploit intelligence summary with exploitation status, risk prediction evaluation, exploit documents analysis, monitoring/telemetry timeline with earliest occurrences, conditional CVE chain analysis only when explicitly evidenced, and risk assessment based on shared database data. The last line must be a single-line RISK_INPUTS JSON object.',
    agent=exploit_researcher,
    context=[research_planning_task, vulnerability_research_task]
)

threat_intelligence_task = Task(
    description="""THREAT INTELLIGENCE RESEARCH: Use internet search as your primary tool to research verified threat intelligence focused on adversary attribution and attack campaigns.

üö® CRITICAL: EXTRACT CVE IDs FROM CONTEXT FIRST üö®
BEFORE searching, you MUST:
1. Review vulnerability_research_task output to extract ALL CVE IDs (e.g., CVE-2025-7775, CVE-2025-7776)
2. Search using CVE IDs, NOT bulletin IDs (bulletin IDs like CTX694938 yield poor results)
3. Example GOOD search: "CVE-2025-7775 exploitation threat actor"
4. Example BAD search: "CTX694938 Citrix advisory threat intelligence"
5. If multiple CVEs present, prioritize the one with active exploitation evidence

TOOL USAGE STRATEGY:
1. CONTEXT-FIRST RULE: Review vulnerability_research_task output to extract CVE IDs before making tool calls
2. INTERNET SEARCH (PRIMARY): Mandatory for threat actor attribution, campaign research, and TTP analysis - USE CVE IDs
3. MCP DATABASE TOOLS: Use for NEW vulnerability identifiers discovered during threat research that are NOT in context
4. NO REDUNDANCY: Never query MCP for CVE/bulletin data already present in context

SEARCH OBJECTIVES (using CVE IDs):
1. THREAT ACTOR ATTRIBUTION: Use internet search to find specific threat actor names with sources
2. ATTACK CAMPAIGNS: Research specific campaign names/identifiers with victim intelligence
3. TTP ANALYSIS - SPECIFIC EVIDENCE REQUIRED:
   - Search for SPECIFIC MITRE ATT&CK techniques with T-codes (e.g., T1190, T1059.001)
   - Search for SPECIFIC exploitation steps (e.g., "CVE-2025-7775 webshell deployment")
   - Search for SPECIFIC tools/malware names (e.g., "Cobalt Strike", "Mimikatz")
   - IF no specific TTPs found ‚Üí Report "No specific TTP data available from sources"
   - DO NOT write vague summaries without T-codes and technique names
4. VICTIM INTELLIGENCE: Search for specific organization names (when publicly disclosed)
5. TIMELINE CORRELATION: Find specific dates of observed exploitation
6. RELATED VULNERABILITIES: If NEW CVE IDs discovered, use MCP tools to retrieve data

OUTPUT REQUIREMENTS:
- IF you find specific TTPs with T-codes ‚Üí Include them with sources
- IF you find vague descriptions without T-codes ‚Üí Report "TTP data insufficient for analysis"
- DO NOT summarize TTPs in generic terms - either provide specifics or state data is unavailable

TTP COLLECTION FOCUS (VIA INTERNET SEARCH):
- Search for specific exploitation techniques observed in the wild
- Document weaponized exploit usage and deployment methods
- Analyze command and control infrastructure associated with exploitation
- Identify persistence mechanisms and lateral movement techniques
- Map to MITRE ATT&CK framework for standardized TTP classification

SOURCE HIERARCHY: Government agencies (CISA, NSA), threat intel firms (CrowdStrike, Mandiant, ShadowServer), security news (THN, BleepingComputer).

VERIFICATION: Cross-reference attributions, distinguish confirmed vs. suspected, focus on actionable intelligence.

OUTPUT PROTOCOL:
- Append exactly one single line at the very end: RISK_INPUTS={...} as specified in your prompt.
""",
    expected_output='Threat intelligence summary with specific TTPs (MITRE T-codes, technique names, tools/malware) when found, or explicit statement that TTP data is unavailable. Include specific threat actor names with confidence levels, campaign identifiers, victim organizations (if disclosed), and exploitation timelines with sources. If TTPs are vague or missing, state "No specific TTP data available" rather than writing generic descriptions. The last line must be a single-line RISK_INPUTS JSON object.',
    agent=threat_intelligence_analyst,
    context=[research_planning_task, vulnerability_research_task, exploit_analysis_task]
)

technical_exploitation_task = Task(
    description="""MANDATORY EXPLOIT RESEARCH: Use available MCP tools to retrieve detailed technical exploitation information for vulnerabilities with exploitation evidence.

üö® CRITICAL ANTI-REDUNDANCY ENFORCEMENT üö®  
BEFORE ANY TOOL CALL: Check if this exact bulletin/CVE ID was ALREADY queried in previous tasks
- If vulnerability_research_task output contains CTX694938 data, DO NOT call bulletin_by_id(CTX694938) again
- If exploit_analysis_task already fetched data for an ID, DO NOT query it again
- ALL tool calls for IDs already in context are STRICTLY FORBIDDEN and will FAIL the task
- Your FIRST action must be: "Reviewing context from previous tasks for existing technical data..."

CRITICAL TOOL USAGE REQUIREMENTS:
- MUST use available MCP tools to gather technical exploit details ONLY for NEW identifiers not in context
- Search for exploit documents, PoC code, and technical methodologies using MCP tools for NEW documents only
- Extract and analyze bulletins and documents ALREADY provided in context data from previous tasks
- Extract code samples, exploitation steps, and technical requirements from context data first

ACTIVATION CRITERIA: Execute for vulnerabilities showing exploitation evidence:
- Active exploitation indicators from provided context data
- Multiple exploit documents available in context data
- CISA KEV or similar catalog presence indicating active exploitation
- Significant technical research needed beyond basic status analysis

TECHNICAL ANALYSIS OBJECTIVES:
1. MCP TOOL RESEARCH (MANDATORY FIRST STEP): Use available MCP tools to retrieve exploit information
   - Use CVE query tools to get CVE details and related exploit documents
   - Use bulletin query tools to retrieve specific exploit bulletins by ID
   - Extract exploitation evidence, document references, and technical indicators from tool responses
   - Parse related documents and bulletin IDs from tool responses

2. EXPLOIT DOCUMENT RETRIEVAL: Use MCP tools to retrieve specific exploit documents
   - Extract document identifiers from tool responses (bulletin IDs, document references)
   - Use bulletin query tools to retrieve specific exploit documents referenced in context data
   - Parse technical details, code samples, and methodologies from retrieved documents
   - Focus on documents containing PoC, exploit code, or technical write-ups

3. CONDITIONAL INTERNET TECHNICAL SEARCH: Only search internet when MCP tool data lacks specific technical exploitation details
   - EXTRACT CVE IDs from context first (e.g., CVE-2025-7775 from CTX694938 bulletin data)
   - Use targeted queries with CVE IDs: "[CVE-ID] exploit technical details methodology", "[CVE-ID] PoC GitHub"
   - Example GOOD: "CVE-2025-7775 exploit PoC GitHub"
   - Example BAD: "CTX694938 exploit technical details" (bulletin IDs yield poor results)
   - Look for proof-of-concept code, exploitation walkthroughs, technical write-ups not present in tool responses
   - Prioritize authoritative sources (GitHub, ExploitDB, security research blogs)
   - Focus on recent publications with technical depth not covered by MCP tools

4. TECHNICAL SYNTHESIS: Extract and organize exploitation details from all sources
   - Exploitation methodology and attack vectors
   - Code snippets and proof-of-concept examples
   - Technical requirements and prerequisites
   - Mitigation bypass techniques
   - Real-world exploitation scenarios
   - Attack chain analysis

5. QUALITY VALIDATION: Assess technical information credibility
   - Source authority and expertise
   - Technical accuracy and completeness
   - Code functionality and reproducibility
   - Recentness and current relevance

DELIVERABLES: Comprehensive technical exploitation summary including:
- Detailed exploitation methodologies with step-by-step analysis
- Technical requirements, prerequisites, and constraints
- Code examples and functional proof-of-concept references
- Attack vectors and exploitation paths with technical details
- Real-world application scenarios and impact analysis
- Technical mitigation analysis and bypass techniques

üö® CRITICAL EFFICIENCY RULES üö®
- MCP TOOLS FIRST: Always use available MCP tools before internet search
- AVOID REDUNDANCY: If previous research already retrieved data, avoid redundant calls for the same information
- USE APPROPRIATE TOOLS: Use bulletin query tools for specific document retrieval, CVE query tools for CVE details
- CONDITIONAL INTERNET: Only use internet search when MCP tool data is insufficient

TOOL USAGE MANDATORY: You MUST use available MCP tools to gather exploit information. Do NOT delegate - perform the research yourself.

OUTPUT PROTOCOL:
- Append exactly one single line at the very end: RISK_INPUTS={...} as specified in your prompt.

OUTPUT FORMAT: Structured technical summary with clear sections for methodology, code examples, and technical analysis.""",
    expected_output='Comprehensive technical exploitation summary including detailed methodologies, code examples, attack vectors, technical requirements, and exploitation analysis for vulnerabilities with ample exploitation evidence. The last line must be a single-line RISK_INPUTS JSON object.',
    agent=technical_exploit_researcher,
    context=[research_planning_task, vulnerability_research_task, exploit_analysis_task]
)

risk_scoring_task = Task(
    description="""CRITICAL RISK SCORING TASK: Generate the quantitative risk score by aggregating ALL RISK_INPUTS from previous tasks. This is MANDATORY and must execute.

DATA EXTRACTION:
The RISK_INPUTS are embedded as the LAST LINE in each previous task's output. Your task context contains the FULL OUTPUT TEXT from all previous tasks.

STEP 1: PARSE PREVIOUS TASK OUTPUTS
Look for lines starting with "RISK_INPUTS={" in the context from:
- research_planning_task output (contains popularity data)
- vulnerability_research_task output (contains exploit evidence)
- exploit_analysis_task output (contains exploitation metrics)
- threat_intelligence_task output (contains document counts)
- technical_exploitation_task output (contains technical constraints)

STEP 2: EXTRACT AND PARSE JSON
For each task output, find the line "RISK_INPUTS={...}" and parse the JSON object.

STEP 3: AGGREGATE ALL RISK_INPUTS
You MUST aggregate all available RISK_INPUTS before computing the final score.

IF YOU DON'T SEE RISK_INPUTS LINES: The previous tasks' outputs are in your context - read through them to find the RISK_INPUTS lines at the end of each output.

RISK SCORING OBJECTIVES:
1. DATA EXTRACTION: Extract key metrics from SHARED research outputs (search flexibly for these data points):
   - Core CVE information: ID, published date, description (any fields with "id", "cve", "published", "date", "description")
   - Risk scoring metrics: CVSS scores and vectors (any fields with "cvss", "score", "vector", "severity", "base_score")
   - Exploit prediction: EPSS/probability scores and percentiles (any fields with "epss", "probability", "percentile", "score")
   - Exploitation evidence: Active exploitation indicators, monitoring data (any fields with "exploit", "wild", "active", "shadowserver", "kev", "cisa", "honeypot", "telemetry")
   - Affected products/systems (any fields with "affected", "product", "vendor", "software", "system")
   - Related documents by type and metadata WITH PUBLICATION DATES (any fields with "document", "bulletin", "advisory", "reference", "published", "date")
   - Weakness classifications: CWE and attack patterns (any fields with "cwe", "weakness", "vulnerability_type", "capec", "attack_pattern")
   - Current date for recency calculations

2. RECENCY EVALUATION: Assess information freshness and current status:
   - Calculate age of all documents in days from current date
   - Identify documents older than 90 days as potentially outdated
   - Check for patches previously marked as "delayed" but now available
   - Flag any stale exploitation information or outdated vendor guidance
   - Determine current patch availability status for risk adjustment

3. EVIDENCE ANALYSIS: Assess exploitation evidence strength from shared data with recency weighting:
   - Active exploitation confirmation and sources (search for exploitation indicator fields)
   - Known Exploited Vulnerabilities catalog presence (search for KEV, CISA, catalog fields)
   - Exploit document availability from document listings (search for GitHub, ExploitDB, PoC, exploit-type documents)
   - Scanner coverage with view counts (search for Nessus, OpenVAS, Nuclei, scanner documents)
   - Vendor advisories and emergency patches (search for vendor, advisory, patch, solution fields)
   - Security research coverage and authority from shared intelligence
   - Apply recency weighting to all evidence types

3. POPULARITY ASSESSMENT: Evaluate affected system popularity from shared product data:
   - Internet-critical infrastructure (Exchange, AD, Linux kernel, OpenSSL): 1.0
   - Enterprise backbone (VMware, cloud platforms, databases): 0.8
   - Business applications (CMS, frameworks, databases): 0.6
   - Specialized enterprise software: 0.3
   - Obscure/niche tools: 0.1

4. TECHNICAL EXPLOITABILITY: Analyze from shared scoring metrics and weakness data:
   - Network accessibility (search for AV:N vs AV:L or similar indicators in CVSS vectors)
   - Attack complexity (search for AC:L vs AC:H or similar indicators)
   - Privileges required (search for PR:N vs PR:L/H or similar indicators)
   - User interaction requirements (search for UI:N vs UI:R or similar indicators)
   - Weakness boost for high-impact categories (search for critical CWE types or attack pattern severity)

5. SCORE COMPUTATION: Apply evidence-based algorithm to shared data:
   - Aggregate exploitation evidence factor (E)
   - Calculate popularity factor (P) with hard caps
   - Determine technical exploitability (T)
   - Integrate EPSS scores (R) with appropriate weighting
   - Apply adaptive scoring formula with context constraints
   - Calculate uncertainty metric based on evidence quality

EFFICIENCY RULE: Work exclusively with previously collected intelligence in your context - NO TOOLS OR DELEGATION NEEDED.

üö® CRITICAL REQUIREMENTS üö®
- Use ONLY data from previous research task outputs (available in your task context)
- Parse the context text to extract RISK_INPUTS lines from each previous task
- NEVER invent or assume missing data points
- If a RISK_INPUTS line is missing from a task, skip it and continue with available data
- Apply scoring algorithm precisely as specified
- Return exactly one JSON object: {"value": X.X, "uncertainty": Y.Y}
- Ensure value is 0.0-10.0 (1 decimal) and uncertainty is 0.0-10.0 (1 decimal)
- Lower uncertainty = higher confidence in the score""",
    expected_output='JSON object with quantitative risk assessment: {"value": X.X, "uncertainty": Y.Y} where value is the 0.0-10.0 risk score and uncertainty is the 0.0-10.0 confidence metric (lower = more confident).',
    agent=risk_analyst,
    context=[research_planning_task, vulnerability_research_task, exploit_analysis_task, threat_intelligence_task, technical_exploitation_task]
)

analysis_task = Task(
    description="""FINAL COMPREHENSIVE SYNTHESIS: You are the FINAL step in the analysis pipeline. Create the complete vulnerability analysis report that integrates ALL previous research findings into a cohesive narrative.

[MANDATORY FINAL ANALYSIS - TERMINAL TASK - DO NOT SKIP]

CRITICAL MANAGER INSTRUCTIONS:
- This is the FINAL and ONLY task that produces the comprehensive report
- ALL previous tasks (planning, research, analysis, intelligence, scoring) MUST complete first
- The manager LLM must ensure this task executes LAST in the workflow
- This produces the END-TO-END complete vulnerability analysis

CRITICAL REQUIREMENTS:
- This is the ONLY task that produces the final comprehensive report
- You MUST execute after ALL other tasks complete
- You MUST integrate the quantitative risk score from the risk_analyst
- You MUST write in FLOWING NARRATIVE PARAGRAPHS (6-8 total), NOT bullet points
- You MUST prominently feature the risk score and uncertainty metrics

üö® CRITICAL: NO GENERIC CYBERSECURITY LANGUAGE üö®
- ABSOLUTELY FORBIDDEN: "ever-evolving landscape", "paramount importance", "critical infrastructure", "cybersecurity threats"
- NO sweeping statements about "threat landscape", "cybersecurity community", "digital transformation"
- NO generic introductions like "In today's interconnected world..." or "As cyber threats continue to evolve..."
- INSTEAD: Start directly with specific vulnerability details, CVSS scores, affected products, exploitation evidence
- Write concrete, evidence-based analysis using actual data from research tasks
- Example BAD: "In the ever-evolving landscape of cybersecurity, the identification and analysis of vulnerabilities in widely used products such as Citrix's NetScaler ADC and Gateway are of paramount importance."
- Example GOOD: "CVE-2025-7775 affects Citrix NetScaler ADC and Gateway with a CVSS score of 9.8. The vulnerability enables memory overflow exploitation through IPv6 services and has been actively exploited in the wild."

MEMORY-ENHANCED SYNTHESIS FEATURES:
1. PATTERN RECOGNITION: Use entity memory to identify similar vulnerabilities, threat actors, and exploitation patterns
2. HISTORICAL CONTEXT: Compare current findings with past analyses of related CVEs or similar technologies
3. TREND ANALYSIS: Identify emerging patterns in threat actor behavior or exploitation techniques
4. INTELLIGENT SYNTHESIS: Leverage short-term memory for context-aware report generation

ADAPTIVE ANALYSIS LOGIC:
- Use memory to identify similar past vulnerabilities and their outcomes
- Compare current risk assessment with historical precedents
- Leverage entity memory for threat actor attribution patterns
- Adapt report depth based on vulnerability severity and historical impact

DATA SOURCE: Access ALL previous task outputs including vulnerability research, exploit analysis, threat intelligence, technical exploitation details, and the quantitative risk score from risk_analyst. Leverage memory context for enhanced analysis.

ENHANCED SYNTHESIS REQUIREMENTS:
1. Integrate exact technical metrics from provided context data (search for CVSS scores, CWE classifications, EPSS predictions with precise values - be flexible about field names)
2. Compile exploitation evidence from shared exploit analysis including monitoring/telemetry timeline and earliest occurrences
3. Include CVE relationship analysis from shared intelligence with memory-informed insights:
   - Document related CVE relationships and connections when sources link them (co-exploitation/combined use) or explicitly describe chains
   - Explain how CVEs are used together in campaigns or toolkits when sources state it; only explain stepwise chains when documented
   - Identify prerequisite vulnerabilities that enable attacks only with explicit citations
   - Map attack pathways leveraging multiple vulnerabilities only when supported by evidence
   - Assess the impact of co-exploitation or chains on overall risk assessment only when relationship evidence exists
   - Use memory to identify similar relationship patterns from past analyses
4. Document remediation steps with exact versions and configurations from shared sources
5. Provide detection mechanisms and monitoring guidance from shared research
6. MANDATORY: Integrate the generated risk score prominently in the analysis
7. CRITICAL: Evaluate information recency and current patch availability status with historical context
8. CONDITIONAL: Include detailed technical exploitation analysis ONLY when Technical Exploitation Analyst provided specific methodologies/code
9. CONDITIONAL: Include TTP analysis ONLY if Threat Intelligence Analyst provided specific MITRE T-codes, technique names, and sources
   - IF threat intelligence contains specific TTPs with T-codes ‚Üí Include TTP section with specifics
   - IF threat intelligence is vague or missing TTPs ‚Üí OMIT TTP section entirely
   - NEVER write placeholder TTP sections like "when available" or "provide understanding"

RECENCY EVALUATION:
- Assess publication dates of all documents and intelligence
- Flag any information older than 90 days as potentially outdated
- Explicitly check if patches mentioned as "delayed" are now available
- Cross-reference document dates with current date to identify stale information
- Prioritize recent patches, advisories, and security updates in recommendations

REPORT FORMAT: Flowing narrative paragraphs (NOT bullet points), 6-8 paragraphs total.
- Opening: Vulnerability overview with key metrics (CVSS scores, CWE classifications, EPSS scores/percentiles) and exploitation status from shared data
- Risk Assessment: Prominently feature the computed risk score (value and uncertainty) with contextual explanation of the scoring methodology and factors
- Current Status Evaluation: Explicit assessment of patch availability and information recency
- Exploitation Analysis: EPSS score and percentile with interpretation (High: >0.7, Medium: 0.4-0.7, Low: <0.4), monitoring/telemetry timeline with earliest occurrences, exploit availability, and technical details from shared analysis
- Technical Exploitation Details: ONLY include if analyst provided specific methodologies/code; otherwise OMIT this section
- Threat Intelligence & TTPs: ONLY include if analyst provided specific threat actors, MITRE T-codes (e.g., T1190), technique names, and sources; otherwise OMIT this section entirely (do not write placeholders)
- CVE Relationship Analysis: Detailed analysis of related CVE relationships, including co-exploitation/combined use, and stepwise exploitation chains ONLY when explicitly evidenced in shared intelligence; otherwise, focus on individual CVEs without assuming chains
- Remediation Guidance: Specific patches and mitigation strategies prioritized by risk score from shared sources, emphasizing current availability
- Closing Assessment: Risk evaluation with score justification and priority recommendations

RISK SCORE INTEGRATION:
- Extract the JSON risk score from the shared risk scoring task output
- Interpret the risk value (0.0-10.0) and uncertainty (0.0-10.0) in context
- Explain how the score was derived from available shared evidence
- Use the score to prioritize remediation recommendations
- Contextualize the uncertainty level and its implications

EFFICIENCY RULE: Work exclusively with previously collected intelligence - no additional queries needed.

QUALITY: Every claim traceable to shared task outputs, no speculation, focus on actionable intelligence, risk score and EPSS metrics prominently featured.

FINAL OUTPUT: This is the END of the analysis pipeline. Do NOT append RISK_INPUTS. Produce only the comprehensive narrative report.""",
    expected_output='FINAL COMPREHENSIVE VULNERABILITY ANALYSIS REPORT: Write 5-8 flowing narrative paragraphs (NO bullet points) that integrate ALL research findings. MUST include: (1) Opening vulnerability overview with key metrics (CVSS, CWE, EPSS score/percentile), (2) Prominent risk score integration with explanation, (3) Current status and recency evaluation, (4) Exploitation analysis with EPSS interpretation and timeline, (5) Technical exploitation details ONLY if specific data provided, (6) Threat intelligence & TTPs ONLY if specific MITRE T-codes and techniques provided, (7) CVE relationship analysis when evidenced, (8) Remediation guidance. OMIT sections without specific evidence - do not write placeholders. The quantitative risk score (value and uncertainty) and EPSS metrics must be prominently featured and explained.',
    agent=analyst,
    context=[research_planning_task, vulnerability_research_task, exploit_analysis_task, threat_intelligence_task, technical_exploitation_task, risk_scoring_task]
)

# =============================================================================
# CREW SETUP AND ORCHESTRATION
# =============================================================================

# Configure memory embedder
MEMORY_EMBEDDER_CONFIG = {
    "provider": "openai",
    "config": {
        "model": "text-embedding-3-small",
        "api_key": os.getenv("OPENAI_API_KEY")
    }
}

# Configure selective memory systems to prevent JSON parsing errors
try:
    from crewai.memory import ShortTermMemory, EntityMemory

    # Create short-term and entity memory instances (long-term disabled)
    # Use default embedder configuration
    short_term_memory = ShortTermMemory()
    entity_memory = EntityMemory()

    print("‚úì Selective memory configuration: Short-term and Entity enabled, Long-term disabled")

except Exception as e:
    print(f"‚ö†Ô∏è Could not configure selective memory: {e}")
    short_term_memory = None
    entity_memory = None

# Function to create a fresh crew for each request (prevents state accumulation issues)
def create_crew():
    """Create a new Crew instance to avoid state accumulation across requests."""
    return Crew(
        name="VM-Agent",
        tasks=[
            research_planning_task,
            vulnerability_research_task,
            exploit_analysis_task,
            threat_intelligence_task,
            technical_exploitation_task,
            risk_scoring_task,
            analysis_task
        ],    
        
        agents=[
             research_planner,
             vulnerability_researcher,
             exploit_researcher,
             threat_intelligence_analyst,
             technical_exploit_researcher,
             risk_analyst,
             analyst
        ],
        
        process=Process.hierarchical,  # CrewAI hierarchical process - manager delegates to agents
        verbose=True,  # Enable detailed logging to see individual agent executions
        max_rpm=None,  # No rate limiting
        share_crew=False,  # Disable telemetry
        manager_llm=manager_llm,  # Dedicated manager LLM (CrewAI auto-creates manager agent)

        # Memory system configuration (CrewAI 1.4)
        # Long-term memory: Disabled to prevent JSON parsing errors
        # Short-term: Enabled for context within current execution
        # Entity: Enabled to track CVEs, threat actors, products
        memory=False,  # Disable automatic memory setup
        short_term_memory=short_term_memory,
        entity_memory=entity_memory,
        long_term_memory=None,  # Explicitly disable long-term memory
        embedder=MEMORY_EMBEDDER_CONFIG
    )

# Create the base crew instance for backwards compatibility
base_crew = create_crew()
